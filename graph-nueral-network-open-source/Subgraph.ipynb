{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st_d10_th0.001_lr0.001\n",
      "Epoch  0\n",
      "Training:  1.109994\n",
      "Validation:  1.0310444\n",
      "Epoch  30\n",
      "Training:  0.7318274\n",
      "Validation:  0.72400457\n",
      "Epoch  60\n",
      "Training:  0.6829689\n",
      "Validation:  0.67807174\n",
      "Epoch  90\n",
      "Training:  0.65736765\n",
      "Validation:  0.65223676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gnn.gnn_utils as gnn_utils\n",
    "from gnn.GNN import GNN as GraphNetwork\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "##### GPU & stuff config\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "data_path = \"./data\"\n",
    "\n",
    "set_name = \"sub_15_7_200\"\n",
    "############# training set ################\n",
    "\n",
    "\n",
    "#inp, arcnode, nodegraph, nodein, labels = Library.set_load_subgraph(data_path, \"train\")\n",
    "inp, arcnode, nodegraph, nodein, labels, _ = gnn_utils.set_load_general(data_path, \"train\", set_name=set_name)\n",
    "############ test set ####################\n",
    "\n",
    "#inp_test, arcnode_test, nodegraph_test, nodein_test, labels_test = Library.set_load_subgraph(data_path, \"test\")\n",
    "inp_test, arcnode_test, nodegraph_test, nodein_test, labels_test, _ = gnn_utils.set_load_general(data_path, \"test\", set_name=set_name)\n",
    "\n",
    "############ validation set #############\n",
    "\n",
    "#inp_val, arcnode_val, nodegraph_val, nodein_val, labels_val = Library.set_load_subgraph(data_path, \"valid\")\n",
    "inp_val, arcnode_val, nodegraph_val, nodein_val, labels_val, _ = gnn_utils.set_load_general(data_path, \"validation\", set_name=set_name)\n",
    "\n",
    "EPSILON = 0.00000001\n",
    "\n",
    "@tf.function()\n",
    "def loss_fcn(target,output):\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    output = tf.maximum(output, EPSILON, name=\"Avoiding_explosions\")  # to avoid explosions\n",
    "    xent = -tf.reduce_sum(target * tf.math.log(output), 1)\n",
    "    lo = tf.reduce_mean(xent)\n",
    "    return lo\n",
    "\n",
    "@tf.function()\n",
    "def metric(output, target):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(target, 1))\n",
    "    metric = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    return metric\n",
    "\n",
    "\n",
    "inp = inp[0]\n",
    "\n",
    "arcnode=arcnode[0]\n",
    "\n",
    "nodegraph=nodegraph[0]\n",
    "\n",
    "inp_val = inp_val[0]\n",
    "\n",
    "arcnode_val = arcnode_val[0]\n",
    "\n",
    "nodegraph_val=nodegraph_val[0]\n",
    "\n",
    "# set input and output dim, the maximum number of iterations, the number of epochs and the optimizer\n",
    "\n",
    "threshold = 0.001\n",
    "learning_rate = 0.001\n",
    "state_dim = 10\n",
    "\n",
    "input_dim = len(inp[0])\n",
    "output_dim = 2\n",
    "max_it = 50\n",
    "num_epoch = 100\n",
    "\n",
    "# initialize GNN\n",
    "param = \"st_d\" + str(state_dim) + \"_th\" + str(threshold) + \"_lr\" + str(learning_rate)\n",
    "print(param)\n",
    "\n",
    "\n",
    "model = GraphNetwork(input_dim, state_dim, output_dim,                             \n",
    "                         hidden_state_dim = 15, hidden_output_dim = 10,\n",
    "                         ArcNode=arcnode,NodeGraph=None,threshold=threshold)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer,loss_fcn)\n",
    "\n",
    "\n",
    "\n",
    "for count in range(0, num_epoch):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = model.train_step(inp.astype(np.float32),labels)\n",
    "\n",
    "\n",
    "        if count % 30 == 0:\n",
    "            out_val = model.predict(inp_val.astype(np.float32), arcnode_val)\n",
    "            loss_value_val = loss_fcn(labels_val,out_val)\n",
    "            \n",
    "            print(\"Epoch \", count)\n",
    "            print(\"Training: \", loss_value.numpy())\n",
    "            print(\"Validation: \",loss_value_val.numpy())\n",
    "\n",
    "        count = count + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcnode_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12/np.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
